{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T06:25:32.471612400Z",
     "start_time": "2024-07-25T06:25:32.305465300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/ArthurSSS9966/SoftHebb.git\n",
    "#%cd SoftHebb\n",
    "\n",
    "import os.path\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Test to load the SoftHebb package\n",
    "from dataset import make_data_loaders\n",
    "from model import load_layers, HebbianOptimizer, AggregateOptim, save_layers\n",
    "from engine import train_sup, evaluate_sup\n",
    "from train import check_dimension, training_config\n",
    "from utils import seed_init_fn, load_presets, load_config_dataset, CustomStepLR\n",
    "from log import Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b073e7f70e70a03",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T06:25:32.659535100Z",
     "start_time": "2024-07-25T06:25:32.413544200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range = 2.165063509461097\n",
      "range = 0.3872983346207417\n",
      "block 0, size : 96 14 14\n",
      "range = 5.0\n",
      "range = 0.12626906806902635\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "params = {\"preset\":\"2SoftMlpMNIST\", \"dataset_sup\":\"MNIST\", \"dataset_unsup\":\"MNIST\",\n",
    "          \"seed\":52,\"model-name\":\"2SoftMlpMNIST\", \"training_mode\":\"simultaneous\", \"training_blocks\":None,\n",
    "          \"resume\":False, \"save\":False}\n",
    "\n",
    "name_model = params[\"preset\"]\n",
    "blocks = load_presets(params[\"preset\"])\n",
    "CNN_blocks = load_presets(\"2SoftHebbCnnCIFAR\") # Disregard the CIFAR, it is there for the name of the preset\n",
    "\n",
    "dataset_sup_config = load_config_dataset(params[\"dataset_sup\"], 0.8)\n",
    "dataset_unsup_config = load_config_dataset(params[\"dataset_unsup\"], 0.8)\n",
    "dataset_sup_config['validation'] = True\n",
    "\n",
    "blocks = check_dimension(blocks, dataset_sup_config)\n",
    "CNN_blocks = check_dimension(CNN_blocks, dataset_sup_config)\n",
    "\n",
    "train_config = training_config(blocks, dataset_sup_config, dataset_unsup_config, params[\"training_mode\"],\n",
    "                               params[\"training_blocks\"])\n",
    "\n",
    "cnn_train_config = training_config(CNN_blocks, dataset_sup_config, dataset_unsup_config, params[\"training_mode\"],\n",
    "                                   params[\"training_blocks\"])\n",
    "\n",
    "config = train_config['t1']\n",
    "\n",
    "train_loader, val_loader, test_loader = make_data_loaders(dataset_sup_config, config['batch_size'], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22696a77c4bbfa5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T06:25:32.790974400Z",
     "start_time": "2024-07-25T06:25:32.647551200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model 2SoftMlpMNIST not found\n",
      "\n",
      "\n",
      "\n",
      " ----- Architecture Block FlattenH78420002, number 0 -----\n",
      "- Flatten(start_dim=1, end_dim=-1)\n",
      "- HebbSoftLinear(in_features=784, out_features=2000, lebesgue_p=2,  bias=False, t_invert=12.0, bias=False, lr_bias=0.0833)\n",
      "- SoftMax(t_invert=5.0, dim=1)\n",
      "\n",
      " ----- Architecture Block Linear(in_, number 1 -----\n",
      "- Linear(in_features=2000, out_features=10, bias=True)\n",
      "\n",
      " Model 2SoftHebbCnnCIFAR not found\n",
      "\n",
      "\n",
      "\n",
      " ----- Architecture Block BatchNorm2dS1962(5, 5)1.0reflect, number 0 -----\n",
      "- BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "- HebbSoftConv2d(1, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, activation=exp)\n",
      "- Triangle(power=0.7)\n",
      "- AvgPool2d(kernel_size=4, stride=2, padding=1)\n",
      "\n",
      " ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 1 -----\n",
      "- Flatten(start_dim=1, end_dim=-1)\n",
      "- Dropout(p=0.5, inplace=False)\n",
      "- Linear(in_features=18816, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "SoftHebb_model = load_layers(blocks, name_model, False)\n",
    "SoftHebb_CNN_model = load_layers(CNN_blocks, \"2SoftHebbCnnCIFAR\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87daa4b8d83ea0a4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T06:25:33.049295200Z",
     "start_time": "2024-07-25T06:25:32.946560700Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        final_epoch: int,\n",
    "        print_freq: int,\n",
    "        lr: float,\n",
    "        folder_name: str,\n",
    "        model,\n",
    "        device,\n",
    "        log,\n",
    "        blocks,\n",
    "        learning_mode: str = 'BP'\n",
    "):\n",
    "    \"\"\"\n",
    "    Hybrid training of one model, happens during simultaneous training mode\n",
    "    \"\"\"\n",
    "    print('\\n', '********** Hybrid learning of blocks %s **********' % blocks)\n",
    "    optimizer_sgd = optim.Adam(\n",
    "        model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    hebbian_optimizer = HebbianOptimizer(model)\n",
    "    scheduler = CustomStepLR(optimizer_sgd, final_epoch)\n",
    "    optimizer = AggregateOptim((hebbian_optimizer, optimizer_sgd))\n",
    "    log_batch = log.new_log_batch()\n",
    "    \n",
    "    val_accuracies = []\n",
    "    storing_path = os.path.join(\"D:\\\\BlcRepo\\OtherCode\\\\NeuroAI\\\\SoftHebb\\\\Models\", folder_name)\n",
    "        \n",
    "    for epoch in range(1, final_epoch + 1):\n",
    "        measures, lr = train_sup(model, criterion, optimizer, train_loader, device, log_batch, learning_mode, blocks)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % print_freq == 0 or epoch == final_epoch or epoch == 1:\n",
    "\n",
    "            loss_test, acc_test = evaluate_sup(model, criterion, val_loader, device)\n",
    "            \n",
    "            log_batch = log.step(epoch, log_batch, loss_test, acc_test, lr, save=True)\n",
    "            log.verbose()\n",
    "            \n",
    "            print(\n",
    "                f'Loss test: {loss_test:.4f}, Accuracy test: {acc_test:.2f}%'\n",
    "            )\n",
    "            \n",
    "            val_accuracies.append(acc_test)\n",
    "        \n",
    "        if os.path.exists(storing_path):\n",
    "            save_layers(model, folder_name, epoch, blocks, storing_path=storing_path)\n",
    "        else:\n",
    "            os.makedirs(storing_path)\n",
    "            save_layers(model, folder_name, epoch, blocks, storing_path=storing_path)\n",
    "            \n",
    "    torch.save(model.state_dict(), os.path.join(storing_path, f\"{folder_name}.pt\")  )  \n",
    "    return log_batch, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e5bad273911d643",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T06:34:51.647836700Z",
     "start_time": "2024-07-25T06:25:33.056459600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********** Hybrid learning of blocks [0, 1] **********\n",
      "Epoch: [1/100]\tlr: 1.00e-03\ttime: 00:00:17\tLoss_train 0.03505\tAcc_train 40.92\t/\tLoss_test 0.03424\tAcc_test 52.28\n",
      "Loss test: 0.0342, Accuracy test: 52.28%\n",
      "Epoch: [10/100]\tlr: 1.00e-03\ttime: 00:01:58\tLoss_train 0.01912\tAcc_train 90.24\t/\tLoss_test 0.00749\tAcc_test 96.35\n",
      "Loss test: 0.0075, Accuracy test: 96.35%\n",
      "Epoch: [20/100]\tlr: 2.50e-04\ttime: 00:03:54\tLoss_train 0.00504\tAcc_train 96.59\t/\tLoss_test 0.00377\tAcc_test 96.53\n",
      "Loss test: 0.0038, Accuracy test: 96.53%\n",
      "Epoch: [30/100]\tlr: 1.25e-04\ttime: 00:05:37\tLoss_train 0.00319\tAcc_train 96.69\t/\tLoss_test 0.00300\tAcc_test 96.62\n",
      "Loss test: 0.0030, Accuracy test: 96.62%\n",
      "Epoch: [40/100]\tlr: 3.13e-05\ttime: 00:07:24\tLoss_train 0.00281\tAcc_train 96.73\t/\tLoss_test 0.00287\tAcc_test 96.48\n",
      "Loss test: 0.0029, Accuracy test: 96.48%\n",
      "Epoch: [50/100]\tlr: 7.81e-06\ttime: 00:09:18\tLoss_train 0.00274\tAcc_train 96.69\t/\tLoss_test 0.00287\tAcc_test 96.54\n",
      "Loss test: 0.0029, Accuracy test: 96.54%\n"
     ]
    }
   ],
   "source": [
    "log = Log(train_config)\n",
    "config = train_config['t1']\n",
    "\n",
    "SoftHebb_loss, SoftHebb_valloss = train_model(\n",
    "                                                50,\n",
    "                                                10,\n",
    "                                                config['lr'],\n",
    "                                                name_model,\n",
    "                                                SoftHebb_model,\n",
    "                                                device,\n",
    "                                                log.sup['t1'],\n",
    "                                                blocks=config['blocks']\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5654634ffb607887",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T06:34:52.026635900Z",
     "start_time": "2024-07-25T06:34:51.647836700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 96.38%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_test, acc_test = evaluate_sup(SoftHebb_model, criterion, test_loader, device)\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {acc_test:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********** Hybrid learning of blocks [0, 1] **********\n",
      "Epoch: [1/100]\tlr: 1.00e-03\ttime: 00:00:31\tLoss_train 0.02246\tAcc_train 84.94\t/\tLoss_test 0.00838\tAcc_test 86.77\n",
      "Loss test: 0.0084, Accuracy test: 86.77%\n",
      "Epoch: [10/100]\tlr: 1.00e-03\ttime: 00:05:14\tLoss_train 0.01481\tAcc_train 89.05\t/\tLoss_test 0.01055\tAcc_test 94.24\n",
      "Loss test: 0.0105, Accuracy test: 94.24%\n"
     ]
    }
   ],
   "source": [
    "log = Log(cnn_train_config)\n",
    "config = cnn_train_config['t1']\n",
    "\n",
    "CnnHebb_loss, CnnHebb_valloss = train_model(\n",
    "                                            50,\n",
    "                                            10,\n",
    "                                            0.001,\n",
    "                                            \"2SoftHebbCnnMNIST\",\n",
    "                                            SoftHebb_CNN_model,\n",
    "                                            device,\n",
    "                                            log.sup['t1'],\n",
    "                                            blocks=config['blocks']\n",
    "                                        )"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-07-25T06:34:52.021648800Z"
    }
   },
   "id": "9eeceeed46286856",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_test, acc_test = evaluate_sup(SoftHebb_CNN_model, criterion, test_loader, device)\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {acc_test:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2e78bf8f146ca3fc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# SoftMNIST = torch.load(\"D:\\\\BlcRepo\\OtherCode\\\\NeuroAI\\\\SoftHebb\\\\Models\\\\2SoftMlpMNIST\\\\2SoftMlpMNIST.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c27d362b82278301",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
