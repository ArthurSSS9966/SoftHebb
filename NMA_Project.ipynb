{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-24T00:16:48.000995700Z",
     "start_time": "2024-07-24T00:16:43.003059800Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "import pandas as pd\n",
    "import bz2\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Test to load the SoftHebb package\n",
    "from SoftHebb.dataset import make_data_loaders\n",
    "from SoftHebb.model import load_layers, save_layers, HebbianOptimizer, AggregateOptim\n",
    "from SoftHebb.engine import train_sup, evaluate_sup\n",
    "from SoftHebb.train import check_dimension, training_config\n",
    "from SoftHebb.utils import seed_init_fn, load_presets, load_config_dataset, CustomStepLR\n",
    "from SoftHebb.log import Log"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range = 2.165063509461097\n",
      "range = 0.3872983346207417\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "params = {\"preset\":\"2SoftMlpMNIST\", \"dataset_sup\":\"MNIST\", \"dataset_unsup\":\"MNIST\",\n",
    "          \"seed\":52,\"model-name\":\"2SoftMlpMNIST\", \"training_mode\":\"simultaneous\", \"training_blocks\":None,\n",
    "          \"resume\":False, \"save\":False}\n",
    "\n",
    "name_model = params[\"preset\"]\n",
    "blocks = load_presets(params[\"preset\"])\n",
    "dataset_sup_config = load_config_dataset(params[\"dataset_sup\"], 0.8)\n",
    "dataset_unsup_config = load_config_dataset(params[\"dataset_unsup\"], 0.8)\n",
    "dataset_sup_config['validation'] = True\n",
    "if params[\"seed\"] is not None:\n",
    "    dataset_sup_config['seed'] = params[\"seed\"]\n",
    "    dataset_unsup_config['seed'] = params[\"seed\"]\n",
    "\n",
    "if dataset_sup_config['seed'] is not None:\n",
    "    seed_init_fn(dataset_sup_config['seed'])\n",
    "\n",
    "blocks = check_dimension(blocks, dataset_sup_config)\n",
    "\n",
    "train_config = training_config(blocks, dataset_sup_config, dataset_unsup_config, params[\"training_mode\"],\n",
    "                               params[\"training_blocks\"])\n",
    "\n",
    "config = train_config['t1']\n",
    "\n",
    "train_loader, val_loader, test_loader = make_data_loaders(dataset_sup_config, config['batch_size'], device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T23:43:47.347545300Z",
     "start_time": "2024-07-23T23:43:47.118248500Z"
    }
   },
   "id": "88badff55475a484",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the ResNet18 model without pretrained weights\n",
    "model = resnet18(weights=None)\n",
    "\n",
    "# Modify the first convolutional layer to accept 1-channel (grayscale) images\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Modify the final fully connected layer to output 10 classes (for MNIST)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T23:24:36.494332900Z",
     "start_time": "2024-07-23T23:24:36.307470100Z"
    }
   },
   "id": "1e35c553548d7ba2",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:: 100%|██████████| 750/750 [03:11<00:00,  3.92it/s]\n",
      "Validation:: 100%|██████████| 188/188 [00:27<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Train Loss: 0.0473, Train Accuracy: 98.60%, Val Loss: 0.0498, Val Accuracy: 98.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:: 100%|██████████| 750/750 [03:03<00:00,  4.09it/s]\n",
      "Validation:: 100%|██████████| 188/188 [00:04<00:00, 40.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Train Loss: 0.0399, Train Accuracy: 98.80%, Val Loss: 0.0469, Val Accuracy: 98.67%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "model.to(device)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss, correct_train, total_train = 0.0, 0.0, 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training:\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss, correct_val, total_val = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation:\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Save checkpoint if validation accuracy improves\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': val_accuracy,\n",
    "        }\n",
    "        with bz2.BZ2File('best_model_checkpoint.pbz2', 'w') as f:\n",
    "            pickle.dump(checkpoint, f)\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Epoch': list(range(1, num_epochs + 1)),\n",
    "    'Train Loss': train_losses,\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Val Loss': val_losses,\n",
    "    'Val Accuracy': val_accuracies\n",
    "})\n",
    "\n",
    "metrics_df.to_csv('training_metrics.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T23:38:43.310176600Z",
     "start_time": "2024-07-23T23:31:39.294996400Z"
    }
   },
   "id": "d1590dffe22438a8",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 2 with validation accuracy: 98.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:: 100%|██████████| 10/10 [00:01<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 99.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the best model checkpoint\n",
    "with bz2.BZ2File('best_model_checkpoint.pbz2', 'rb') as f:\n",
    "    checkpoint = pickle.load(f)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "best_val_accuracy = checkpoint['val_accuracy']\n",
    "\n",
    "print(f'Loaded checkpoint from epoch {epoch+1} with validation accuracy: {best_val_accuracy:.2f}%')\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing:\") :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T23:39:01.329599400Z",
     "start_time": "2024-07-23T23:38:54.920394900Z"
    }
   },
   "id": "4ca72dc032af6eda",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model 2SoftMlpMNIST not found\n",
      "\n",
      "\n",
      "\n",
      " ----- Architecture Block FlattenH78420002, number 0 -----\n",
      "- Flatten(start_dim=1, end_dim=-1)\n",
      "- HebbSoftLinear(in_features=784, out_features=2000, lebesgue_p=2,  bias=False, t_invert=12.0, bias=False, lr_bias=0.0833)\n",
      "- SoftMax(t_invert=5.0, dim=1)\n",
      "\n",
      " ----- Architecture Block Linear(in_, number 1 -----\n",
      "- Linear(in_features=2000, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "SoftHebb_model = load_layers(blocks, name_model, False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T23:43:55.773955500Z",
     "start_time": "2024-07-23T23:43:55.580551600Z"
    }
   },
   "id": "58ae7d22ff55f49b",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        final_epoch: int,\n",
    "        print_freq: int,\n",
    "        lr: float,\n",
    "        folder_name: str,\n",
    "        model,\n",
    "        device,\n",
    "        log,\n",
    "        blocks,\n",
    "        learning_mode: str = 'BP',\n",
    "        save_batch: bool = True,\n",
    "        save: bool = True,\n",
    "        report=None,\n",
    "        plot_fc=None,\n",
    "        model_dir=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Hybrid training of one model, happens during simultaneous training mode\n",
    "    \"\"\"\n",
    "\n",
    "    print('\\n', '********** Hybrid learning of blocks %s **********' % blocks)\n",
    "\n",
    "    optimizer_sgd = optim.Adam(\n",
    "        model.parameters(), lr=lr)  # , weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    hebbian_optimizer = HebbianOptimizer(model)\n",
    "    scheduler = CustomStepLR(optimizer_sgd, final_epoch)\n",
    "    optimizer = AggregateOptim((hebbian_optimizer, optimizer_sgd))\n",
    "    log_batch = log.new_log_batch()\n",
    "    \n",
    "    for epoch in range(1, final_epoch + 1):\n",
    "        measures, lr = train_sup(model, criterion, optimizer, train_loader, device, log_batch, learning_mode, blocks)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % print_freq == 0 or epoch == final_epoch or epoch == 1:\n",
    "\n",
    "            loss_test, acc_test = evaluate_sup(model, criterion, val_loader, device)\n",
    "\n",
    "            log_batch = log.step(epoch, log_batch, loss_test, acc_test, lr, save=save_batch)\n",
    "\n",
    "            if report is not None:\n",
    "                _, train_loss, train_acc, test_loss, test_acc = log.data[-1]\n",
    "\n",
    "                conv, R1 = model.convergence()\n",
    "                report(train_loss=train_loss, train_acc=train_acc, test_loss=test_loss, test_acc=test_acc,\n",
    "                       convergence=conv, R1=R1)\n",
    "\n",
    "            else:\n",
    "                log.verbose()\n",
    "            if save:\n",
    "                save_layers(model, folder_name, epoch, blocks, storing_path=model_dir)\n",
    "\n",
    "            if plot_fc is not None:\n",
    "                for block in blocks:\n",
    "                    plot_fc(model, block)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T00:07:22.969460600Z",
     "start_time": "2024-07-24T00:07:22.833908100Z"
    }
   },
   "id": "3bd37b7884d410b7",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********** Hybrid learning of blocks [0, 1] **********\n",
      "Epoch: [1/100]\tlr: 1.00e-03\ttime: 00:00:12\tLoss_train 0.03502\tAcc_train 41.64\t/\tLoss_test 0.00218\tAcc_test 54.81\n",
      "Epoch: [50/100]\tlr: 2.50e-04\ttime: 00:08:35\tLoss_train 0.00540\tAcc_train 95.69\t/\tLoss_test 0.00012\tAcc_test 96.60\n",
      "Epoch: [100/100]\tlr: 7.81e-06\ttime: 00:17:17\tLoss_train 0.00157\tAcc_train 96.94\t/\tLoss_test 0.00012\tAcc_test 96.63\n"
     ]
    }
   ],
   "source": [
    "log = Log(train_config)\n",
    "config = train_config['t1']\n",
    "\n",
    "train_model(\n",
    "    config['nb_epoch'],\n",
    "    config['print_freq'],\n",
    "    config['lr'],\n",
    "    name_model,\n",
    "    SoftHebb_model,\n",
    "    device,\n",
    "    log.sup['t1'],\n",
    "    blocks=config['blocks'],\n",
    "    save=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T00:02:42.517279900Z",
     "start_time": "2024-07-23T23:45:24.997161400Z"
    }
   },
   "id": "b90b2dfd0c64f0d5",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_model(\n",
    "    config['nb_epoch'],\n",
    "    config['print_freq'],\n",
    "    config['lr'],\n",
    "    'ResNet18',\n",
    "    model,\n",
    "    device,\n",
    "    log.sup['t1'],\n",
    "    blocks=config['blocks'],\n",
    "    save=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93bd79a628f6a636"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 96.63%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_test, acc_test = evaluate_sup(SoftHebb_model, criterion, test_loader, device)\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {acc_test:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T00:03:08.403910800Z",
     "start_time": "2024-07-24T00:03:07.975449900Z"
    }
   },
   "id": "3fabfa1ea8e016d5",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "510a5e48882e7abb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
