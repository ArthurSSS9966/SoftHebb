{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T07:02:10.675058400Z",
     "start_time": "2024-07-25T07:02:10.559874200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/ArthurSSS9966/SoftHebb.git\n",
    "#%cd SoftHebb\n",
    "\n",
    "import os.path\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Test to load the SoftHebb package\n",
    "from dataset import make_data_loaders\n",
    "from model import load_layers, HebbianOptimizer, AggregateOptim, save_layers\n",
    "from engine import train_sup, evaluate_sup\n",
    "from train import check_dimension, training_config\n",
    "from utils import seed_init_fn, load_presets, load_config_dataset, CustomStepLR\n",
    "from log import Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b073e7f70e70a03",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T07:02:13.652893600Z",
     "start_time": "2024-07-25T07:02:13.423601900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range = 2.165063509461097\n",
      "range = 1.7320508075688772\n",
      "block 0, size : 100 14 14\n",
      "range = 5.0\n",
      "range = 0.12371791482634838\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "params = {\"preset\":\"2SoftMlpMNIST\", \"dataset_sup\":\"MNIST\", \"dataset_unsup\":\"MNIST\",\n",
    "          \"seed\":52,\"model-name\":\"2SoftMlpMNIST\", \"training_mode\":\"simultaneous\", \"training_blocks\":None,\n",
    "          \"resume\":False, \"save\":False}\n",
    "\n",
    "name_model = params[\"preset\"]\n",
    "blocks = load_presets(params[\"preset\"])\n",
    "CNN_blocks = load_presets(\"2SoftHebbCnnCIFAR\") # Disregard the CIFAR, it is there for the name of the preset\n",
    "\n",
    "dataset_sup_config = load_config_dataset(params[\"dataset_sup\"], 0.8)\n",
    "dataset_unsup_config = load_config_dataset(params[\"dataset_unsup\"], 0.8)\n",
    "dataset_sup_config['validation'] = True\n",
    "\n",
    "blocks = check_dimension(blocks, dataset_sup_config)\n",
    "CNN_blocks = check_dimension(CNN_blocks, dataset_sup_config)\n",
    "\n",
    "train_config = training_config(blocks, dataset_sup_config, dataset_unsup_config, params[\"training_mode\"],\n",
    "                               params[\"training_blocks\"])\n",
    "\n",
    "cnn_train_config = training_config(CNN_blocks, dataset_sup_config, dataset_unsup_config, params[\"training_mode\"],\n",
    "                                   params[\"training_blocks\"])\n",
    "\n",
    "config = train_config['t1']\n",
    "\n",
    "train_loader, val_loader, test_loader = make_data_loaders(dataset_sup_config, config['batch_size'], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e22696a77c4bbfa5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T07:02:15.182407300Z",
     "start_time": "2024-07-25T07:02:15.055133200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model 2SoftMlpMNIST not found\n",
      "\n",
      "\n",
      "\n",
      " ----- Architecture Block FlattenH7841002, number 0 -----\n",
      "- Flatten(start_dim=1, end_dim=-1)\n",
      "- HebbSoftLinear(in_features=784, out_features=100, lebesgue_p=2,  bias=False, t_invert=12.0, bias=False, lr_bias=0.0833)\n",
      "- SoftMax(t_invert=5.0, dim=1)\n",
      "\n",
      " ----- Architecture Block Linear(in_, number 1 -----\n",
      "- Linear(in_features=100, out_features=10, bias=True)\n",
      "\n",
      " Model 2SoftHebbCnnCIFAR not found\n",
      "\n",
      "\n",
      "\n",
      " ----- Architecture Block BatchNorm2dS11002(5, 5)1.0reflect, number 0 -----\n",
      "- BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "- HebbSoftConv2d(1, 100, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, activation=exp)\n",
      "- Triangle(power=0.7)\n",
      "- AvgPool2d(kernel_size=4, stride=2, padding=1)\n",
      "\n",
      " ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 1 -----\n",
      "- Flatten(start_dim=1, end_dim=-1)\n",
      "- Dropout(p=0.5, inplace=False)\n",
      "- Linear(in_features=19600, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "SoftHebb_model = load_layers(blocks, name_model, False)\n",
    "SoftHebb_CNN_model = load_layers(CNN_blocks, \"2SoftHebbCnnCIFAR\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87daa4b8d83ea0a4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T07:02:16.825352200Z",
     "start_time": "2024-07-25T07:02:16.713113600Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        final_epoch: int,\n",
    "        print_freq: int,\n",
    "        lr: float,\n",
    "        folder_name: str,\n",
    "        model,\n",
    "        device,\n",
    "        log,\n",
    "        blocks,\n",
    "        learning_mode: str = 'BP'\n",
    "):\n",
    "    \"\"\"\n",
    "    Hybrid training of one model, happens during simultaneous training mode\n",
    "    \"\"\"\n",
    "    print('\\n', '********** Hybrid learning of blocks %s **********' % blocks)\n",
    "    optimizer_sgd = optim.Adam(\n",
    "        model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    hebbian_optimizer = HebbianOptimizer(model)\n",
    "    scheduler = CustomStepLR(optimizer_sgd, final_epoch)\n",
    "    optimizer = AggregateOptim((hebbian_optimizer, optimizer_sgd))\n",
    "    log_batch = log.new_log_batch()\n",
    "    \n",
    "    val_accuracies = []\n",
    "    storing_path = os.path.join(\"D:\\\\BlcRepo\\OtherCode\\\\NeuroAI\\\\SoftHebb\\\\Models\", folder_name)\n",
    "        \n",
    "    for epoch in range(1, final_epoch + 1):\n",
    "        measures, lr = train_sup(model, criterion, optimizer, train_loader, device, log_batch, learning_mode, blocks)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % print_freq == 0 or epoch == final_epoch or epoch == 1:\n",
    "\n",
    "            loss_test, acc_test = evaluate_sup(model, criterion, val_loader, device)\n",
    "            \n",
    "            log_batch = log.step(epoch, log_batch, loss_test, acc_test, lr, save=True)\n",
    "            log.verbose()\n",
    "            \n",
    "            print(\n",
    "                f'Loss test: {loss_test:.4f}, Accuracy test: {acc_test:.2f}%'\n",
    "            )\n",
    "            \n",
    "            val_accuracies.append(acc_test)\n",
    "        \n",
    "    if os.path.exists(storing_path):\n",
    "        save_layers(model, folder_name, final_epoch, blocks, storing_path=storing_path)\n",
    "    else:\n",
    "        os.makedirs(storing_path)\n",
    "        save_layers(model, folder_name, final_epoch, blocks, storing_path=storing_path)\n",
    "            \n",
    "    torch.save(model, os.path.join(storing_path, f\"{folder_name}.pt\")  )  \n",
    "    return log_batch, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e5bad273911d643",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T07:06:48.223397900Z",
     "start_time": "2024-07-25T07:02:19.278159700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********** Hybrid learning of blocks [0, 1] **********\n",
      "Epoch: [1/100]\tlr: 1.00e-03\ttime: 00:00:05\tLoss_train 0.03194\tAcc_train 65.14\t/\tLoss_test 0.02694\tAcc_test 88.12\n",
      "Loss test: 0.0269, Accuracy test: 88.12%\n",
      "Epoch: [10/100]\tlr: 1.00e-03\ttime: 00:00:47\tLoss_train 0.01038\tAcc_train 88.70\t/\tLoss_test 0.00545\tAcc_test 90.04\n",
      "Loss test: 0.0054, Accuracy test: 90.04%\n",
      "Epoch: [20/100]\tlr: 2.50e-04\ttime: 00:01:44\tLoss_train 0.00564\tAcc_train 88.77\t/\tLoss_test 0.00540\tAcc_test 89.15\n",
      "Loss test: 0.0054, Accuracy test: 89.15%\n",
      "Epoch: [30/100]\tlr: 1.25e-04\ttime: 00:02:37\tLoss_train 0.00569\tAcc_train 88.84\t/\tLoss_test 0.00607\tAcc_test 88.03\n",
      "Loss test: 0.0061, Accuracy test: 88.03%\n",
      "Epoch: [40/100]\tlr: 3.13e-05\ttime: 00:03:39\tLoss_train 0.00665\tAcc_train 87.12\t/\tLoss_test 0.00658\tAcc_test 87.31\n",
      "Loss test: 0.0066, Accuracy test: 87.31%\n",
      "Epoch: [50/100]\tlr: 7.81e-06\ttime: 00:04:28\tLoss_train 0.00719\tAcc_train 86.07\t/\tLoss_test 0.00729\tAcc_test 86.04\n",
      "Loss test: 0.0073, Accuracy test: 86.04%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'generator' object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m log \u001B[38;5;241m=\u001B[39m Log(train_config)\n\u001B[0;32m      2\u001B[0m config \u001B[38;5;241m=\u001B[39m train_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt1\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m----> 4\u001B[0m SoftHebb_loss, SoftHebb_valloss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                                                \u001B[49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                                                \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mname_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mSoftHebb_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mlog\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mt1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mblocks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mblocks\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[11], line 52\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(final_epoch, print_freq, lr, folder_name, model, device, log, blocks, learning_mode)\u001B[0m\n\u001B[0;32m     49\u001B[0m         os\u001B[38;5;241m.\u001B[39mmakedirs(storing_path)\n\u001B[0;32m     50\u001B[0m         save_layers(model, folder_name, epoch, blocks, storing_path\u001B[38;5;241m=\u001B[39mstoring_path)\n\u001B[1;32m---> 52\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstoring_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mfolder_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.pt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m  \u001B[49m\u001B[43m)\u001B[49m  \n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m log_batch, val_accuracies\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\neuroAI\\lib\\site-packages\\torch\\serialization.py:628\u001B[0m, in \u001B[0;36msave\u001B[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[0m\n\u001B[0;32m    626\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[1;32m--> 628\u001B[0m         \u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_disable_byteorder_record\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\neuroAI\\lib\\site-packages\\torch\\serialization.py:840\u001B[0m, in \u001B[0;36m_save\u001B[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001B[0m\n\u001B[0;32m    838\u001B[0m pickler \u001B[38;5;241m=\u001B[39m pickle_module\u001B[38;5;241m.\u001B[39mPickler(data_buf, protocol\u001B[38;5;241m=\u001B[39mpickle_protocol)\n\u001B[0;32m    839\u001B[0m pickler\u001B[38;5;241m.\u001B[39mpersistent_id \u001B[38;5;241m=\u001B[39m persistent_id\n\u001B[1;32m--> 840\u001B[0m \u001B[43mpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    841\u001B[0m data_value \u001B[38;5;241m=\u001B[39m data_buf\u001B[38;5;241m.\u001B[39mgetvalue()\n\u001B[0;32m    842\u001B[0m zip_file\u001B[38;5;241m.\u001B[39mwrite_record(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m, data_value, \u001B[38;5;28mlen\u001B[39m(data_value))\n",
      "\u001B[1;31mTypeError\u001B[0m: cannot pickle 'generator' object"
     ]
    }
   ],
   "source": [
    "log = Log(train_config)\n",
    "config = train_config['t1']\n",
    "\n",
    "SoftHebb_loss, SoftHebb_valloss = train_model(\n",
    "                                                50,\n",
    "                                                10,\n",
    "                                                config['lr'],\n",
    "                                                name_model,\n",
    "                                                SoftHebb_model,\n",
    "                                                device,\n",
    "                                                log.sup['t1'],\n",
    "                                                blocks=config['blocks']\n",
    "                                            )\n",
    "# Save the model\n",
    "torch.save({'state_dict': SoftHebb_model.state_dict(),\n",
    "        'config': SoftHebb_model.config}, os.path.join(\"D:\\\\BlcRepo\\OtherCode\\\\NeuroAI\\\\SoftHebb\\\\Models\", name_model, f\"{name_model}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5654634ffb607887",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T07:33:27.484270600Z",
     "start_time": "2024-07-25T07:33:27.174011900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 85.84%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_test, acc_test = evaluate_sup(SoftHebb_model, criterion, test_loader, device)\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {acc_test:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "log = Log(cnn_train_config)\n",
    "config = cnn_train_config['t1']\n",
    "\n",
    "CnnHebb_loss, CnnHebb_valloss = train_model(\n",
    "                                            50,\n",
    "                                            10,\n",
    "                                            0.001,\n",
    "                                            \"2SoftHebbCnnMNIST\",\n",
    "                                            SoftHebb_CNN_model,\n",
    "                                            device,\n",
    "                                            log.sup['t1'],\n",
    "                                            blocks=config['blocks']\n",
    "                                        )\n",
    "\n",
    "# Save the model\n",
    "torch.save({'state_dict': SoftHebb_CNN_model.state_dict(),\n",
    "        'config': SoftHebb_CNN_model.config}, os.path.join(\"D:\\\\BlcRepo\\OtherCode\\\\NeuroAI\\\\SoftHebb\\\\Models\", \"2SoftHebbCnnMNIST\", f\"2SoftHebbCnnMNIST.pth\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eeceeed46286856",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_test, acc_test = evaluate_sup(SoftHebb_CNN_model, criterion, test_loader, device)\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {acc_test:.2f}%')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e78bf8f146ca3fc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Examples of how to load pre-trained models\n",
    "\n",
    "# # Load the model\n",
    "# from model import MultiLayer\n",
    "# \n",
    "# SoftMNIST = torch.load(\"path//to//model//xxx.pth\")\n",
    "# \n",
    "# SoftMNIST_Model = MultiLayer(SoftMNIST['config'])\n",
    "# SoftMNIST_Model.load_state_dict(SoftMNIST['state_dict'])\n",
    "# \n",
    "# SoftMNIST_Model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T07:42:21.758092100Z",
     "start_time": "2024-07-25T07:42:21.638306Z"
    }
   },
   "id": "c27d362b82278301",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T07:42:21.793312800Z",
     "start_time": "2024-07-25T07:42:21.763385Z"
    }
   },
   "id": "d9af846060d10190",
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
