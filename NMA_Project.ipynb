{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "import pandas as pd\n",
    "import bz2\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Test to load the SoftHebb package\n",
    "from SoftHebb.dataset import make_data_loaders\n",
    "from SoftHebb.model import load_layers, save_layers, HebbianOptimizer, AggregateOptim\n",
    "from SoftHebb.engine import train_sup, evaluate_sup\n",
    "from SoftHebb.train import check_dimension, training_config\n",
    "from SoftHebb.utils import seed_init_fn, load_presets, load_config_dataset, CustomStepLR\n",
    "from SoftHebb.log import Log"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "params = {\"preset\":\"2SoftMlpMNIST\", \"dataset_sup\":\"MNIST\", \"dataset_unsup\":\"MNIST\",\n",
    "          \"seed\":52,\"model-name\":\"2SoftMlpMNIST\", \"training_mode\":\"simultaneous\", \"training_blocks\":None,\n",
    "          \"resume\":False, \"save\":False}\n",
    "\n",
    "name_model = params[\"preset\"]\n",
    "blocks = load_presets(params[\"preset\"])\n",
    "dataset_sup_config = load_config_dataset(params[\"dataset_sup\"], 0.8)\n",
    "dataset_unsup_config = load_config_dataset(params[\"dataset_unsup\"], 0.8)\n",
    "dataset_sup_config['validation'] = True\n",
    "if params[\"seed\"] is not None:\n",
    "    dataset_sup_config['seed'] = params[\"seed\"]\n",
    "    dataset_unsup_config['seed'] = params[\"seed\"]\n",
    "\n",
    "if dataset_sup_config['seed'] is not None:\n",
    "    seed_init_fn(dataset_sup_config['seed'])\n",
    "\n",
    "blocks = check_dimension(blocks, dataset_sup_config)\n",
    "\n",
    "train_config = training_config(blocks, dataset_sup_config, dataset_unsup_config, params[\"training_mode\"],\n",
    "                               params[\"training_blocks\"])\n",
    "\n",
    "config = train_config['t1']\n",
    "\n",
    "train_loader, val_loader, test_loader = make_data_loaders(dataset_sup_config, config['batch_size'], device)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b073e7f70e70a03"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the ResNet18 model without pretrained weights\n",
    "model = resnet18(weights=None)\n",
    "\n",
    "# Modify the first convolutional layer to accept 1-channel (grayscale) images\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Modify the final fully connected layer to output 10 classes (for MNIST)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8cc1a54fffeb957"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "model.to(device)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss, correct_train, total_train = 0.0, 0.0, 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training:\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss, correct_val, total_val = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation:\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Save checkpoint if validation accuracy improves\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': val_accuracy,\n",
    "        }\n",
    "        with bz2.BZ2File('best_model_checkpoint.pbz2', 'w') as f:\n",
    "            pickle.dump(checkpoint, f)\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Epoch': list(range(1, num_epochs + 1)),\n",
    "    'Train Loss': train_losses,\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Val Loss': val_losses,\n",
    "    'Val Accuracy': val_accuracies\n",
    "})\n",
    "\n",
    "metrics_df.to_csv('training_metrics.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fe98aa03a256b8c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the best model checkpoint\n",
    "with bz2.BZ2File('best_model_checkpoint.pbz2', 'rb') as f:\n",
    "    checkpoint = pickle.load(f)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "best_val_accuracy = checkpoint['val_accuracy']\n",
    "\n",
    "print(f'Loaded checkpoint from epoch {epoch+1} with validation accuracy: {best_val_accuracy:.2f}%')\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing:\") :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d99a995a179dbbfa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "SoftHebb_model = load_layers(blocks, name_model, False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e22696a77c4bbfa5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        final_epoch: int,\n",
    "        print_freq: int,\n",
    "        lr: float,\n",
    "        folder_name: str,\n",
    "        model,\n",
    "        device,\n",
    "        log,\n",
    "        blocks,\n",
    "        learning_mode: str = 'BP',\n",
    "        save_batch: bool = True,\n",
    "        save: bool = True,\n",
    "        report=None,\n",
    "        plot_fc=None,\n",
    "        model_dir=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Hybrid training of one model, happens during simultaneous training mode\n",
    "    \"\"\"\n",
    "\n",
    "    print('\\n', '********** Hybrid learning of blocks %s **********' % blocks)\n",
    "\n",
    "    optimizer_sgd = optim.Adam(\n",
    "        model.parameters(), lr=lr)  # , weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    hebbian_optimizer = HebbianOptimizer(model)\n",
    "    scheduler = CustomStepLR(optimizer_sgd, final_epoch)\n",
    "    optimizer = AggregateOptim((hebbian_optimizer, optimizer_sgd))\n",
    "    log_batch = log.new_log_batch()\n",
    "    \n",
    "    for epoch in range(1, final_epoch + 1):\n",
    "        measures, lr = train_sup(model, criterion, optimizer, train_loader, device, log_batch, learning_mode, blocks)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % print_freq == 0 or epoch == final_epoch or epoch == 1:\n",
    "\n",
    "            loss_test, acc_test = evaluate_sup(model, criterion, val_loader, device)\n",
    "\n",
    "            log_batch = log.step(epoch, log_batch, loss_test, acc_test, lr, save=save_batch)\n",
    "\n",
    "            if report is not None:\n",
    "                _, train_loss, train_acc, test_loss, test_acc = log.data[-1]\n",
    "\n",
    "                conv, R1 = model.convergence()\n",
    "                report(train_loss=train_loss, train_acc=train_acc, test_loss=test_loss, test_acc=test_acc,\n",
    "                       convergence=conv, R1=R1)\n",
    "\n",
    "            else:\n",
    "                log.verbose()\n",
    "            if save:\n",
    "                save_layers(model, folder_name, epoch, blocks, storing_path=model_dir)\n",
    "\n",
    "            if plot_fc is not None:\n",
    "                for block in blocks:\n",
    "                    plot_fc(model, block)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87daa4b8d83ea0a4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "log = Log(train_config)\n",
    "config = train_config['t1']\n",
    "\n",
    "train_model(\n",
    "    config['nb_epoch'],\n",
    "    config['print_freq'],\n",
    "    config['lr'],\n",
    "    name_model,\n",
    "    SoftHebb_model,\n",
    "    device,\n",
    "    log.sup['t1'],\n",
    "    blocks=config['blocks'],\n",
    "    save=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e5bad273911d643"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_test, acc_test = evaluate_sup(SoftHebb_model, criterion, test_loader, device)\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {acc_test:.2f}%')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5654634ffb607887"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
