{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T08:01:01.131651900Z",
     "start_time": "2024-07-25T08:01:01.029925900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/ArthurSSS9966/SoftHebb.git\n",
    "#%cd SoftHebb\n",
    "\n",
    "import os.path\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Test to load the SoftHebb package\n",
    "from dataset import make_data_loaders\n",
    "from model import load_layers, HebbianOptimizer, AggregateOptim, save_layers\n",
    "from engine import train_sup, evaluate_sup\n",
    "from train import check_dimension, training_config\n",
    "from utils import seed_init_fn, load_presets, load_config_dataset, CustomStepLR\n",
    "from log import Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b073e7f70e70a03",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:01:01.389874800Z",
     "start_time": "2024-07-25T08:01:01.131651900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range = 2.165063509461097\n",
      "range = 1.7320508075688772\n",
      "block 0, size : 100 14 14\n",
      "range = 5.0\n",
      "range = 0.12371791482634838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\BlcRepo\\OtherCode\\NeuroAI\\SoftHebb\\dataset.py:698: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = torch.tensor(self.data, dtype=torch.float, device=device).div_(255).unsqueeze(1)\n",
      "D:\\BlcRepo\\OtherCode\\NeuroAI\\SoftHebb\\dataset.py:700: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.targets = torch.tensor(self.targets, device=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "params = {\"preset\":\"2SoftMlpMNIST\", \"dataset_sup\":\"MNIST\", \"dataset_unsup\":\"MNIST\",\n",
    "          \"seed\":52,\"model-name\":\"2SoftMlpMNIST\", \"training_mode\":\"simultaneous\", \"training_blocks\":None,\n",
    "          \"resume\":False, \"save\":False}\n",
    "\n",
    "name_model = params[\"preset\"]\n",
    "blocks = load_presets(params[\"preset\"])\n",
    "CNN_blocks = load_presets(\"2SoftHebbCnnCIFAR\") # Disregard the CIFAR, it is there for the name of the preset\n",
    "\n",
    "dataset_sup_config = load_config_dataset(params[\"dataset_sup\"], 0.8)\n",
    "dataset_unsup_config = load_config_dataset(params[\"dataset_unsup\"], 0.8)\n",
    "dataset_sup_config['validation'] = True\n",
    "\n",
    "blocks = check_dimension(blocks, dataset_sup_config)\n",
    "CNN_blocks = check_dimension(CNN_blocks, dataset_sup_config)\n",
    "\n",
    "train_config = training_config(blocks, dataset_sup_config, dataset_unsup_config, params[\"training_mode\"],\n",
    "                               params[\"training_blocks\"])\n",
    "\n",
    "cnn_train_config = training_config(CNN_blocks, dataset_sup_config, dataset_unsup_config, params[\"training_mode\"],\n",
    "                                   params[\"training_blocks\"])\n",
    "\n",
    "config = train_config['t1']\n",
    "\n",
    "train_loader, val_loader, test_loader = make_data_loaders(dataset_sup_config, config['batch_size'], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22696a77c4bbfa5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:01:01.512780300Z",
     "start_time": "2024-07-25T08:01:01.388877900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model 2SoftMlpMNIST not found\n",
      "\n",
      "\n",
      "\n",
      " ----- Architecture Block FlattenH7841002, number 0 -----\n",
      "- Flatten(start_dim=1, end_dim=-1)\n",
      "- HebbSoftLinear(in_features=784, out_features=100, lebesgue_p=2,  bias=False, t_invert=12.0, bias=False, lr_bias=0.0833)\n",
      "- SoftMax(t_invert=5.0, dim=1)\n",
      "\n",
      " ----- Architecture Block Linear(in_, number 1 -----\n",
      "- Linear(in_features=100, out_features=10, bias=True)\n",
      "\n",
      " Model 2SoftHebbCnnCIFAR not found\n",
      "\n",
      "\n",
      "\n",
      " ----- Architecture Block BatchNorm2dS11002(5, 5)1.0reflect, number 0 -----\n",
      "- BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "- HebbSoftConv2d(1, 100, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, activation=exp)\n",
      "- Triangle(power=0.7)\n",
      "- AvgPool2d(kernel_size=4, stride=2, padding=1)\n",
      "\n",
      " ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 1 -----\n",
      "- Flatten(start_dim=1, end_dim=-1)\n",
      "- Dropout(p=0.5, inplace=False)\n",
      "- Linear(in_features=19600, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "SoftHebb_model = load_layers(blocks, name_model, False)\n",
    "SoftHebb_CNN_model = load_layers(CNN_blocks, \"2SoftHebbCnnCIFAR\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87daa4b8d83ea0a4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:01:01.605532500Z",
     "start_time": "2024-07-25T08:01:01.520758800Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        final_epoch: int,\n",
    "        print_freq: int,\n",
    "        lr: float,\n",
    "        folder_name: str,\n",
    "        model,\n",
    "        device,\n",
    "        log,\n",
    "        blocks,\n",
    "        learning_mode: str = 'BP'\n",
    "):\n",
    "    \"\"\"\n",
    "    Hybrid training of one model, happens during simultaneous training mode\n",
    "    \"\"\"\n",
    "    print('\\n', '********** Hybrid learning of blocks %s **********' % blocks)\n",
    "    optimizer_sgd = optim.Adam(\n",
    "        model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    hebbian_optimizer = HebbianOptimizer(model)\n",
    "    scheduler = CustomStepLR(optimizer_sgd, final_epoch)\n",
    "    optimizer = AggregateOptim((hebbian_optimizer, optimizer_sgd))\n",
    "    log_batch = log.new_log_batch()\n",
    "    \n",
    "    val_accuracies = []\n",
    "    storing_path = os.path.join(\"D:\\\\BlcRepo\\OtherCode\\\\NeuroAI\\\\SoftHebb\\\\Models\", folder_name)\n",
    "        \n",
    "    for epoch in range(1, final_epoch + 1):\n",
    "        measures, lr = train_sup(model, criterion, optimizer, train_loader, device, log_batch, learning_mode, blocks)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % print_freq == 0 or epoch == final_epoch or epoch == 1:\n",
    "\n",
    "            loss_test, acc_test = evaluate_sup(model, criterion, val_loader, device)\n",
    "            \n",
    "            log_batch = log.step(epoch, log_batch, loss_test, acc_test, lr, save=True)\n",
    "            log.verbose()\n",
    "            \n",
    "            print(\n",
    "                f'Loss test: {loss_test:.4f}, Accuracy test: {acc_test:.2f}%'\n",
    "            )\n",
    "            \n",
    "            val_accuracies.append(acc_test)\n",
    "        \n",
    "    return log_batch, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e5bad273911d643",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:01:10.102451300Z",
     "start_time": "2024-07-25T08:01:01.607075700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********** Hybrid learning of blocks [0, 1] **********\n",
      "Epoch: [1/100]\tlr: 1.00e-03\ttime: 00:00:05\tLoss_train 0.03194\tAcc_train 65.14\t/\tLoss_test 0.02694\tAcc_test 88.12\n",
      "Loss test: 0.0269, Accuracy test: 88.12%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m log \u001B[38;5;241m=\u001B[39m Log(train_config)\n\u001B[0;32m      2\u001B[0m config \u001B[38;5;241m=\u001B[39m train_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt1\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m----> 4\u001B[0m SoftHebb_loss, SoftHebb_valloss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                                                \u001B[49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                                                \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mname_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mSoftHebb_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mlog\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mt1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mblocks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mblocks\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Save the model\u001B[39;00m\n\u001B[0;32m     15\u001B[0m torch\u001B[38;5;241m.\u001B[39msave({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstate_dict\u001B[39m\u001B[38;5;124m'\u001B[39m: SoftHebb_model\u001B[38;5;241m.\u001B[39mstate_dict(),\n\u001B[0;32m     16\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m'\u001B[39m: SoftHebb_model\u001B[38;5;241m.\u001B[39mconfig}, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mBlcRepo\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mOtherCode\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mNeuroAI\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mSoftHebb\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mModels\u001B[39m\u001B[38;5;124m\"\u001B[39m, name_model, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname_model\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "Cell \u001B[1;32mIn[7], line 28\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(final_epoch, print_freq, lr, folder_name, model, device, log, blocks, learning_mode)\u001B[0m\n\u001B[0;32m     25\u001B[0m storing_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mBlcRepo\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mOtherCode\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mNeuroAI\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mSoftHebb\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mModels\u001B[39m\u001B[38;5;124m\"\u001B[39m, folder_name)\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, final_epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 28\u001B[0m     measures, lr \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_sup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblocks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m scheduler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     31\u001B[0m         scheduler\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mD:\\BlcRepo\\OtherCode\\NeuroAI\\SoftHebb\\engine.py:142\u001B[0m, in \u001B[0;36mtrain_sup\u001B[1;34m(model, criterion, optimizer, loader, device, measures, learning_mode, blocks)\u001B[0m\n\u001B[0;32m    140\u001B[0m         measures, lr, info, convergence, R1 \u001B[38;5;241m=\u001B[39m train_sup_hebb(model, loader, device, measures, criterion)\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 142\u001B[0m         measures, lr \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_BP\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m measures, lr\n",
      "File \u001B[1;32mD:\\BlcRepo\\OtherCode\\NeuroAI\\SoftHebb\\engine.py:12\u001B[0m, in \u001B[0;36mtrain_BP\u001B[1;34m(model, criterion, optimizer, loader, device, measures)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# with torch.autograd.set_detect_anomaly(True):\u001B[39;00m\n\u001B[0;32m     11\u001B[0m t \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, target \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;66;03m## 1. forward propagation$\u001B[39;00m\n\u001B[0;32m     14\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(device, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     15\u001B[0m     target \u001B[38;5;241m=\u001B[39m target\u001B[38;5;241m.\u001B[39mto(device, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\neuroAI\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\neuroAI\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\neuroAI\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\neuroAI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    256\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\neuroAI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\neuroAI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\neuroAI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    144\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\neuroAI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    211\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    212\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 213\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "log = Log(train_config)\n",
    "config = train_config['t1']\n",
    "\n",
    "SoftHebb_loss, SoftHebb_valloss = train_model(\n",
    "                                                50,\n",
    "                                                10,\n",
    "                                                config['lr'],\n",
    "                                                name_model,\n",
    "                                                SoftHebb_model,\n",
    "                                                device,\n",
    "                                                log.sup['t1'],\n",
    "                                                blocks=config['blocks']\n",
    "                                            )\n",
    "# Save the model\n",
    "torch.save({'state_dict': SoftHebb_model.state_dict(),\n",
    "        'config': SoftHebb_model.config}, os.path.join(\"D:\\\\BlcRepo\\OtherCode\\\\NeuroAI\\\\SoftHebb\\\\Models\", name_model, f\"{name_model}.pth\"))dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654634ffb607887",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:01:10.125386100Z",
     "start_time": "2024-07-25T08:01:10.107436100Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_test, acc_test = evaluate_sup(SoftHebb_model, criterion, test_loader, device)\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {acc_test:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********** Hybrid learning of blocks [0, 1] **********\n",
      "Epoch: [1/100]\tlr: 1.00e-03\ttime: 00:00:47\tLoss_train 0.02186\tAcc_train 84.76\t/\tLoss_test 0.00838\tAcc_test 86.32\n",
      "Loss test: 0.0084, Accuracy test: 86.32%\n",
      "Epoch: [10/100]\tlr: 1.00e-03\ttime: 00:07:33\tLoss_train 0.01536\tAcc_train 88.81\t/\tLoss_test 0.01542\tAcc_test 91.47\n",
      "Loss test: 0.0154, Accuracy test: 91.47%\n",
      "Epoch: [20/100]\tlr: 2.50e-04\ttime: 00:15:12\tLoss_train 0.01405\tAcc_train 91.39\t/\tLoss_test 0.00651\tAcc_test 95.23\n",
      "Loss test: 0.0065, Accuracy test: 95.23%\n",
      "Epoch: [30/100]\tlr: 1.25e-04\ttime: 00:22:55\tLoss_train 0.01142\tAcc_train 91.80\t/\tLoss_test 0.00595\tAcc_test 95.28\n",
      "Loss test: 0.0060, Accuracy test: 95.28%\n",
      "Epoch: [40/100]\tlr: 3.13e-05\ttime: 00:30:56\tLoss_train 0.01086\tAcc_train 91.59\t/\tLoss_test 0.00564\tAcc_test 95.41\n",
      "Loss test: 0.0056, Accuracy test: 95.41%\n",
      "Epoch: [50/100]\tlr: 7.81e-06\ttime: 00:41:12\tLoss_train 0.01049\tAcc_train 91.73\t/\tLoss_test 0.00555\tAcc_test 95.40\n",
      "Loss test: 0.0056, Accuracy test: 95.40%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'generator' object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m log \u001B[38;5;241m=\u001B[39m Log(cnn_train_config)\n\u001B[0;32m      2\u001B[0m config \u001B[38;5;241m=\u001B[39m cnn_train_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt1\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m----> 4\u001B[0m CnnHebb_loss, CnnHebb_valloss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                                            \u001B[49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                                            \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m                                            \u001B[49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                                            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m2SoftHebbCnnMNIST\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mSoftHebb_CNN_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mlog\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mt1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mblocks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mblocks\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Save the model\u001B[39;00m\n\u001B[0;32m     16\u001B[0m torch\u001B[38;5;241m.\u001B[39msave({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstate_dict\u001B[39m\u001B[38;5;124m'\u001B[39m: SoftHebb_CNN_model\u001B[38;5;241m.\u001B[39mstate_dict(),\n\u001B[0;32m     17\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m'\u001B[39m: SoftHebb_CNN_model\u001B[38;5;241m.\u001B[39mconfig}, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mBlcRepo\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mOtherCode\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mNeuroAI\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mSoftHebb\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mModels\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2SoftHebbCnnMNIST\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2SoftHebbCnnMNIST.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "Cell \u001B[1;32mIn[7], line 52\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(final_epoch, print_freq, lr, folder_name, model, device, log, blocks, learning_mode)\u001B[0m\n\u001B[0;32m     49\u001B[0m     os\u001B[38;5;241m.\u001B[39mmakedirs(storing_path)\n\u001B[0;32m     50\u001B[0m     save_layers(model, folder_name, final_epoch, blocks, storing_path\u001B[38;5;241m=\u001B[39mstoring_path)\n\u001B[1;32m---> 52\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstoring_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mfolder_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.pt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m  \u001B[49m\u001B[43m)\u001B[49m  \n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m log_batch, val_accuracies\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\neuroAI\\lib\\site-packages\\torch\\serialization.py:628\u001B[0m, in \u001B[0;36msave\u001B[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[0m\n\u001B[0;32m    626\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[1;32m--> 628\u001B[0m         \u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_disable_byteorder_record\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\neuroAI\\lib\\site-packages\\torch\\serialization.py:840\u001B[0m, in \u001B[0;36m_save\u001B[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001B[0m\n\u001B[0;32m    838\u001B[0m pickler \u001B[38;5;241m=\u001B[39m pickle_module\u001B[38;5;241m.\u001B[39mPickler(data_buf, protocol\u001B[38;5;241m=\u001B[39mpickle_protocol)\n\u001B[0;32m    839\u001B[0m pickler\u001B[38;5;241m.\u001B[39mpersistent_id \u001B[38;5;241m=\u001B[39m persistent_id\n\u001B[1;32m--> 840\u001B[0m \u001B[43mpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    841\u001B[0m data_value \u001B[38;5;241m=\u001B[39m data_buf\u001B[38;5;241m.\u001B[39mgetvalue()\n\u001B[0;32m    842\u001B[0m zip_file\u001B[38;5;241m.\u001B[39mwrite_record(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m, data_value, \u001B[38;5;28mlen\u001B[39m(data_value))\n",
      "\u001B[1;31mTypeError\u001B[0m: cannot pickle 'generator' object"
     ]
    }
   ],
   "source": [
    "log = Log(cnn_train_config)\n",
    "config = cnn_train_config['t1']\n",
    "\n",
    "CnnHebb_loss, CnnHebb_valloss = train_model(\n",
    "                                            50,\n",
    "                                            10,\n",
    "                                            0.001,\n",
    "                                            \"2SoftHebbCnnMNIST\",\n",
    "                                            SoftHebb_CNN_model,\n",
    "                                            device,\n",
    "                                            log.sup['t1'],\n",
    "                                            blocks=config['blocks']\n",
    "                                        )\n",
    "\n",
    "# Save the model\n",
    "torch.save({'state_dict': SoftHebb_CNN_model.state_dict(),\n",
    "        'config': SoftHebb_CNN_model.config}, os.path.join(\"D:\\\\BlcRepo\\OtherCode\\\\NeuroAI\\\\SoftHebb\\\\Models\", \"2SoftHebbCnnMNIST\", f\"2SoftHebbCnnMNIST.pth\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:42:27.427140400Z",
     "start_time": "2024-07-25T08:01:14.658920400Z"
    }
   },
   "id": "9eeceeed46286856",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 95.56%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_test, acc_test = evaluate_sup(SoftHebb_CNN_model, criterion, test_loader, device)\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {acc_test:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:45:22.648684300Z",
     "start_time": "2024-07-25T08:45:20.086262300Z"
    }
   },
   "id": "2e78bf8f146ca3fc",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Examples of how to load pre-trained models\n",
    "\n",
    "# Load the model\n",
    "from model import MultiLayer\n",
    "\n",
    "SoftMNIST = torch.load(\"D:\\\\BlcRepo\\OtherCode\\\\NeuroAI\\\\SoftHebb\\\\Models\\\\2SoftMlpMNIST\\\\2SoftMlpMNIST.pth\")\n",
    "\n",
    "SoftMNIST_Model = MultiLayer(SoftMNIST['config'])\n",
    "SoftMNIST_Model.load_state_dict(SoftMNIST['state_dict'])\n",
    "\n",
    "SoftMNIST_Model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-25T08:01:10.128982700Z"
    }
   },
   "id": "c27d362b82278301",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-25T08:01:10.133055600Z"
    }
   },
   "id": "d9af846060d10190",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
